{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44455aad-0d50-4a8d-816a-6551e2e91035",
   "metadata": {},
   "source": [
    "### Download pretrained LLM from [Index of /reimers/sentence-transformers/v0.2/](https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fcea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download and extraction successful.\n"
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "# download_url=\"https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/average_word_embeddings_glove.6B.300d.zip\"\n",
    "# destination_path=\"/home/jovyan/work/glove\"\n",
    "# zip_filename=\"average_word_embeddings_glove.6B.300d.zip\"\n",
    "\n",
    "# # Create the destination directory if it does not exist\n",
    "# mkdir -p \"$destination_path\"\n",
    "\n",
    "# # Download the file quietly\n",
    "# wget -q -O \"$destination_path/$zip_filename\" \"$download_url\"\n",
    "\n",
    "# # Check if the download was successful\n",
    "# if [ -f \"$destination_path/$zip_filename\" ]; then\n",
    "#     # Unzip the file quietly\n",
    "#     unzip -q \"$destination_path/$zip_filename\" -d \"$destination_path\"\n",
    "    \n",
    "#     # Check if the unzip was successful\n",
    "#     if [ $? -eq 0 ]; then\n",
    "#         echo \"Download and extraction successful.\"\n",
    "#     else\n",
    "#         echo \"Extraction failed.\"\n",
    "#     fi\n",
    "    \n",
    "#     # Remove the zip file after extraction\n",
    "#     rm \"$destination_path/$zip_filename\"\n",
    "# else\n",
    "#     echo \"Download failed.\"\n",
    "# fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12c1388-9960-406e-925a-f7903e1d3d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download and extraction successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Define variables\n",
    "download_url = \"https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/all-MiniLM-L6-v2.zip\"\n",
    "destination_path = \"/home/jovyan/work/all-MiniLM-L6-v2\"\n",
    "zip_filename = \"all-MiniLM-L6-v2.zip\"\n",
    "zip_filepath = os.path.join(destination_path, zip_filename)\n",
    "\n",
    "# Create the destination directory if it does not exist\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "# Download the file quietly\n",
    "download_command = f\"wget -q -O {zip_filepath} {download_url}\"\n",
    "download_result = subprocess.run(download_command, shell=True)\n",
    "\n",
    "# Check if the download was successful\n",
    "if os.path.isfile(zip_filepath):\n",
    "    # Unzip the file quietly\n",
    "    unzip_command = f\"unzip -q {zip_filepath} -d {destination_path}\"\n",
    "    unzip_result = subprocess.run(unzip_command, shell=True)\n",
    "    \n",
    "    # Check if the unzip was successful\n",
    "    if unzip_result.returncode == 0:\n",
    "        print(\"Download and extraction successful.\")\n",
    "    else:\n",
    "        print(\"Extraction failed.\")\n",
    "    \n",
    "    # Remove the zip file after extraction\n",
    "    os.remove(zip_filepath)\n",
    "else:\n",
    "    print(\"Download failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7379389e-7a15-4dc7-aed6-e0b02f24c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# default is in ~/.cache\n",
    "# os.environ['HF_HOME'] = '/home/jovyan/cache/'\n",
    "# os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba9b56-2d4c-4864-ae2c-aad98feefae9",
   "metadata": {},
   "source": [
    "### reload from cache snapshot (need to locate the config.json location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76db61a-6225-4a60-9464-a870afb57865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL='thenlper/gte-small'\n",
    "# MODEL='/home/jovyan/cache/hub/models--thenlper--gte-small/snapshots/17e1f347d17fe144873b1201da91788898c639cd'\n",
    "# gte_model = SentenceTransformer(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "531412dd-62c1-4064-b71c-5930c024d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "from sentence_transformers import models, util, datasets, evaluation, losses\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb1c98b-2465-4107-97c1-571188ff0bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL='thenlper/gte-small'\n",
    "MODEL = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(MODEL)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e89a3f-f892-4428-bc45-4254ab46a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4dc1b0-c758-4dda-b446-6b82e6b12ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MODEL = 'all-mpneimport os\n",
    "# os.environ['HF_HOME'] = '/blabla/cache/'t-base-v2'\n",
    "# MODEL = r'../all-MiniLM-L6-v2'\n",
    "\n",
    "\n",
    "\n",
    "################# LOAD SENTENCE TRANSFORMER MODEL ###########################\n",
    "# Load the embedding model and tokenizer manually\n",
    "\n",
    "word_embedding_model = models.Transformer(MODEL)\n",
    "\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "# Assemble the sentence transformer model\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "MAX_SEQ_LENGTH = 25\n",
    "\n",
    "model[0].max_seq_length = MAX_SEQ_LENGTH\n",
    "model[0].tokenizer.max_seq_length = MAX_SEQ_LENGTH\n",
    "model[0].do_lower_case = True\n",
    "model[1].pooling_mode_cls_token=True\n",
    "model[1].pooling_mode_mean_tokens=False\n",
    "model[1].pooling_mode_max_tokens=False\n",
    "model[1].pooling_mode_mean_sqrt_len_tokens=False\n",
    "model[1].pooling_mode_weightedmean_tokens=False\n",
    "model[1].pooling_mode_lasttoken=False\n",
    "\n",
    "print(f\"{model=}\")\n",
    "\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "# model = SentenceTransformer('../all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n",
    "# Save the model locally\n",
    "model.save(r'../gte-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9814f4f-7651-4767-ba7a-62b16f016e9a",
   "metadata": {},
   "source": [
    "### Using Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d88e05-7036-4307-8a56-f8d14405b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r',,/all-MiniLM-L6-v2')\n",
    "model = AutoModel.from_pretrained(r'../all-MiniLM-L6-v2')\n",
    "\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "print(f\"{encoded_input=}\")\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "    print(f\"{model_output['last_hidden_state'].shape=}\")\n",
    "    print(f\"{model_output['pooler_output'].shape=}\")\n",
    "# Perform pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Normalize embeddings\n",
    "sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "print(f\"{sentence_embeddings.shape=}\")\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c040a-6f0a-451b-b73d-bdd03250ec1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
